### 强化学习

##### day 1

###### 简介

   强化学习是一种学习如何将状态映射到动作，以获得最大奖励的学习机制。 学习者不会被告知要采取哪些动作，而是必须通过尝试来发现哪些动作会产生最大的回报。 在最有趣和最具挑战性的案例中，动作不仅可以影响直接奖励，还可以影响下一个状态，并通过下一个状态，影响到随后而来的奖励。 这两个特征 - 试错法和延迟奖励 - 是强化学习的两个最重要的可区别特征。

  与非监督学习的区别：强化学习也与机器学习研究人员所谓的 *无监督学习* 不同，后者通常是用于寻找隐藏在未标记数据集合中的结构。

强化学习四要素：策略，奖励信号，价值函数，和可选的环境模型。

策略：从感知的环境状态到在这些状态下要采取的行动的映射，是强化学习个体的核心，因为它本身就足以确定行为。

奖励信号：定义了强化学习中的目标，是改变策略的主要依据。

价值函数：指定了长期收益，是对个体从该状态开始对未来收益的一个预期，奖励是环境状态对个体的直接反馈。

环境模型：用于规划，在实际行动前对未来的一个预判，给定状态和动作，可以预测下一状态和下一奖励。

井字棋实例：

![image-20211125150248461](C:\Users\longyuan\AppData\Roaming\Typora\typora-user-images\image-20211125150248461.png)

###### 多臂赌博问题

问题：你可以反复面对 kk 种不同的选择或行动。在每次选择之后，你会收到一个数值奖励，该奖励取决于你选择的行动的固定概率分布。 你的目标是在一段时间内最大化预期的总奖励。

想要找出最优解，主要就是要平衡探索和利用之间的需求关系。

 ε 贪婪方法： 在大多数情况下贪婪地行动，但每隔一段时间，以较小的概率 εε， 从动作价值估计中独立地从具有相同概率的所有动作中随机选择

实例 10臂赌博机实验：

![image-20211125153509257](C:\Users\longyuan\AppData\Roaming\Typora\typora-user-images\image-20211125153509257.png)

![image-20211125153543441](C:\Users\longyuan\AppData\Roaming\Typora\typora-user-images\image-20211125153543441.png)

ε 贪婪方法优于贪婪方法的优势取决于任务，若奖励方差较大（奖励更嘈杂），则需要更多的探索才能找到最佳动作，但若奖励方差为0，贪婪方法在尝试一次后就知道每个动作的真是价值，这种情况下贪婪方法表现得会更佳。

增量公式：新估计=旧估计+步长（目标-旧估计）

![image-20211125155607245](C:\Users\longyuan\AppData\Roaming\Typora\typora-user-images\image-20211125155607245.png)

 函数bandit(a) 代表采取行动并返回相应的奖励。每次循环一次就更新一次新的估计值。

UCB行动选择算法：

![image-20211125161931538](C:\Users\longyuan\AppData\Roaming\Typora\typora-user-images\image-20211125161931538.png)



平方根项是对一个值估计的不确定性或方差的度量。 因此，最大化的数量是动作a的可能真实值的一种上限，其中c确定置信水平。 每次选择a时，不确定性可能会降低：Nt(a)递增，并且，正如它在分母中出现的那样，不确定性项减少。 另一方面，每次选择除a之外的动作时，t增加但Nt(a) 不增加；因为t出现在分子中，不确定性估计值会增加。 使用自然对数意味着随着时间的推移，增加量会变小，但是是无限制； 最终将选择所有操作，但是将随着时间的推移，具有较低值估计值或已经频繁选择的操作的选择频率会降低。

赌博机梯度算法：

引入了Ht(a)——偏好。偏好越大，采取行动的次数越多，但偏好在奖励方面没有解释。只有一种行为相对于另一种行为的相对偏好才是重要的。还引入了一个有用的新符号，πt(a)，表示在时间t采取行动的概率。

![image-20211125165725214](C:\Users\longyuan\AppData\Roaming\Typora\typora-user-images\image-20211125165725214.png)

在每个步骤中， 在选择动作 At 并接收奖励 Rt 之后，动作偏好通过以下方式更新：

![image-20211125165838174](C:\Users\longyuan\AppData\Roaming\Typora\typora-user-images\image-20211125165838174.png)

其中 α>0α>0 是步长参数，R¯t∈(R) 是所有奖励的平均值,也叫奖励的基准线， 包括时间t。如果奖励高于基线，那么将来获取 At 的概率增加; 如果奖励低于基线，则概率降低，未选择的动作向相反方向移动。

###### 有限马尔可夫决策过程

MDP是顺序决策的经典形式化，其中行动不仅影响直接奖励，还影响后续情况或状态，以及贯穿未来的奖励。MDP旨在直接构建从交互中学习以实现目标的问题。 学习者和决策者被称为个体（agent）。它与之交互的东西，包括个体之外的所有东西，被称为 *环境*。 这些交互持续不断，个体选择动作同时环境响应那些动作并向个体呈现新情况 。 环境还产生奖励，这是个体通过其行动选择寻求最大化的特殊数值。

![image-20211125172009362](C:\Users\longyuan\AppData\Roaming\Typora\typora-user-images\image-20211125172009362.png)

四参数动力学函数p：s′∈S 和 r∈R， 在给定前一状态和动作的特定值的情况下，存在这些值在时间t发生的概率：

![image-20211125174712400](C:\Users\longyuan\AppData\Roaming\Typora\typora-user-images\image-20211125174712400.png)

根据这个函数，我们可以进行多个变形：

![image-20211125174845499](C:\Users\longyuan\AppData\Roaming\Typora\typora-user-images\image-20211125174845499.png)

MDP框架是抽象和灵活的，可以以不同的方式应用在很多不同的问题上，它的关键就在于个体及其环境之间来回传递的三个信号：一个信号表示个体做出的选择（动作），一个信号表是做出选择的基础（状态），以及另一个信号来定义个体的目标（奖励）。

示例：

![image-20211125180739622](C:\Users\longyuan\AppData\Roaming\Typora\typora-user-images\image-20211125180739622.png)

回报：奖励的总和

![image-20211125183009857](C:\Users\longyuan\AppData\Roaming\Typora\typora-user-images\image-20211125183009857.png)

 γ 是参数，0≤γ≤1，称为 *衰减因子*。γ接近0，个体只关注最大化奖励，会”短视“，γ接近1，回报目标更加强烈地考虑了未来的回报，个体变得更有”远见“

