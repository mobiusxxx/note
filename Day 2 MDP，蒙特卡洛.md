#### Day 2 

###### MDP

策略与价值函数

策略是从状态到选择每个可能动作的概率映射。

在状态 s 下，策略 π 下的 *价值函数* 表示为 vπ(s) ， 是从 s 开始并且之后遵循策略 π 的预期收益。

![image-20211126094945550](C:\Users\longyuan\AppData\Roaming\Typora\typora-user-images\image-20211126094945550.png)

其中 E[]˙ 表示个体遵循策略π 的随机变量的期望值，t是任意的时间步长。

同样，我们定义在策略 π，状态 s 下采取动作 a 的价值， 表示为 qπ(s,a)：

![image-20211126095239420](C:\Users\longyuan\AppData\Roaming\Typora\typora-user-images\image-20211126095239420.png)

贝尔曼方程：它表达了状态价值与下一状态价值之间的关系。对于任何策略 π 和任何状态 s，s 的值与其可能的后继状态的值之间保持以下一致性条件：

![image-20211126101241128](C:\Users\longyuan\AppData\Roaming\Typora\typora-user-images\image-20211126101241128.png)

 它指出，开始状态的值必须等于预期的下一个状态的（衰减）值，加上沿途预期的奖励。

![image-20211126101904300](C:\Users\longyuan\AppData\Roaming\Typora\typora-user-images\image-20211126101904300.png)

实例：

![image-20211126104151146](C:\Users\longyuan\AppData\Roaming\Typora\typora-user-images\image-20211126104151146.png)

最优策略和最优价值函数

最优策略:价值函数对策略进行部分排序。 如果策略 π 所有状态的预期返回值大于或等于策略 π′ 的值， 则该策略 π 被定义为优于或等于策略 π′。

*最优状态价值函数*，表示为 v∗，并定义为:

![image-20211126104538135](C:\Users\longyuan\AppData\Roaming\Typora\typora-user-images\image-20211126104538135.png)

最优策略还具有相同的最优动作价值函数，表示为 q∗，并定义为

![image-20211126104642614](C:\Users\longyuan\AppData\Roaming\Typora\typora-user-images\image-20211126104642614.png)

![image-20211126111813833](C:\Users\longyuan\AppData\Roaming\Typora\typora-user-images\image-20211126111813833.png)

优化和近似

​     在实际生活中，想要找到最优策略，需要极高的计算成本，内存消耗也是一个需要考虑的问题。 在具有小的有限状态集的任务中，可以使用对于每个状态（或状态-动作对）具有一个条目的数组或表来形成这些近似。 这个我们称之为 *表格* 的情况，相应的方法称之为表格方法。然而，在许多实际感兴趣的情况下，还有更多的状态不可能在一个表格中完整表达。 在这些情况下，必须使用某种更紧凑的参数化函数表示来近似函数。

总结

​    强化学习是从互动中学习如何行动从而实现目标。 强化学习 *个体* 及其 *环境* 通过一系列离散的时间步长进行交互。 其接口的规范定义了一个特定的任务：*动作* 是由个体所做的选择；*状态* 是做出选择的基础；*奖励* 是评估选择的基础。 个体内的一切都是由个体完全知晓和控制的；外面的一切都是不完全可控的，可能完全知道也可能不完全知道的。 *策略* 是随机规则，个体通过该规则选择动作作为状态的函数。 个体的目标是随着时间的推移最大限度地获得奖励。

​    在给定个体使用策略的情况下，策略的 *价值函数* 为每个状态或状态-动作对分配该状态或状态-动作对的预期回报。 *最优价值函数* 为每个状态或状态-动作对分配任何策略可实现的最大预期回报。价值函数最优的策略是 *最优策略*。

###### 动态规划

动态规划（DP）这个术语是指可以用于在给定完整的环境模型是马尔可夫决策过程（MDP）的情况下计算最优策略的算法集合，其目的就是是将贝尔曼方程转换为更新规则以此来提升价值函数的近似效果。

策略评估

![image-20211126151123760](C:\Users\longyuan\AppData\Roaming\Typora\typora-user-images\image-20211126151123760.png)

策略提升

 通过根据初始策略的值函数使策略变贪婪的方式来制定一个新的策略提升初始策略的过程，叫做 *策略提升*。计算某个策略价值函数的目的是找到一个更好的策略。我们知道在当前状态 s 遵从当前的策略有多好——也就是vπ(s)——但是改变为一个新的状态会更好还是坏呢？ 一种解决这个问题的方法是考虑从状态 s 下选择动作 a，然后遵从现有的策略 π。这种方式的值是：

![image-20211126151512601](C:\Users\longyuan\AppData\Roaming\Typora\typora-user-images\image-20211126151512601.png)

关键的标准是这等式是大于还是小于vπ。如果是大于——也就是说， 在状态 s 选择执行一次动作 a ，然后遵从策略 π 是会比一直遵从策略 π 好——然后我们当然愿意每次到达状态 s 选择动作 a 都会比较好。 那么新的策略事实上总体来说也会比较好。

策略迭代

![image-20211126152332848](C:\Users\longyuan\AppData\Roaming\Typora\typora-user-images\image-20211126152332848.png)

一步步通过策略评估，策略提升，知道发现最优策略的过程，就是策略迭代。

![image-20211126152306097](C:\Users\longyuan\AppData\Roaming\Typora\typora-user-images\image-20211126152306097.png)

价值迭代

策略迭代的缺点就是每次的迭代过程都会进行策略评估。价值迭代就是去截断策略评估而不影响其收敛性，方法就是策略评估在一次迭代之后就停止（每个状态只有一个回溯）。

![image-20211126164506609](C:\Users\longyuan\AppData\Roaming\Typora\typora-user-images\image-20211126164506609.png)

对所有的 s∈S。 对于任意的 v0，在保证 v∗ 存在的同样的条件下:math:{v_k} 序列可以收敛到 v∗。

![image-20211126164630096](C:\Users\longyuan\AppData\Roaming\Typora\typora-user-images\image-20211126164630096.png)

异步动态规划

*异步* DP算法是就地迭代DP算法，并没有按照规则的状态集更新步骤进行组织。 这些算法以任何顺序更新状态值，使用恰好可用的其他状态值。某些状态的值可能会在其他状态的值更新一次之前被更新多次。

广义策略迭代（GPI）

指代策略评估和策略提升相互交互的一般概念，而不依赖于两个过程的粒度和其他细节。 几乎所有的强化学习方法都可以被描述为GPI。GPI中的评估和提升的过程可以认为是既存在竞争又存在合作。在竞争这个意义上他们走向相反的方向。

![img](https://rl.qiwihui.com/zh_CN/latest/_images/two_lines.png)

动态规划的效率

DP有时候被认为应用有限，因为 *维数灾难* ，状态的数量随着状态变量的增加成指数增长。 非常大的状态集合的确会带来困难，但是这些是问题的固有困难，并不是DP作为一个解决问题的算法本身带来的困难。 实际上，DP方法比相关的直接搜索和线性规划方法在解决大状态空间的问题上要更加合适。

总结

动态规划可以用来解决有限MDP。 *策略评估* 指的是（通常）迭代计算一个给定策略的价值函数。*策略提升* 指的是给定一个策略的价值函数计算一个提升的策略。 将这两种计算放在一起，就会得到策略迭代和价值迭代这两个最流行的DP方法。 在完全了解MDP的情况下，任何一个都能可靠地计算有限MDP的最优策略和价值函数。 *异步* DP方法是用任意顺序进行回溯的就地迭代方法，可能是随机确定的并且使用过时的信息。 很多这些方法可以被看作是更加细粒度的GPI。

###### 蒙特卡洛方法

蒙特卡洛方法是基于对样本回报求平均的办法来解决强化学习的问题的。 为了保证能够得到良好定义的回报，这里我们定义蒙特卡洛方法仅适用于回合制任务。蒙特卡洛方法因此能够写成逐个回合的增量形式，而不是逐步（在线）的形式。

蒙特卡洛方法对比与之前的赌博算法类似，主要区别是蒙特卡洛算法解决的问题具有多种状态，是非平稳问题。为了解决非平稳性，结合了GPI算法，依靠对MDP的了解来 *计算* 价值函数，这里我们从MDP的抽样回报中 *学习* 价值函数。 我们使用相同的办法去获得最优的价值函数和策略，即GPI中价值函数和对应的策略交互作用。 

###### 蒙特卡洛预测

在给定策略的情况下，用蒙特卡洛方法学习状态-价值函数，也就是对经验中的所有的这个状态的回报求平均。在同一个回合中状态 s 可能被访问多次，我们称第一次为 s 的 *首次访问*。 所以我们有两种蒙特卡洛方法，一种只计算所有回合中首次访问状态 s 的平均回报， 以此作为 vπ(s) 的估计值，我们称之为 *首次访问MC方法* ； 与之对应的，另一种方法计算所有回合中每次访问状态 s 的平均回报，我们称之为 *每次访问MC方法* 。每次访问MC方法和首次访问MC方法是相同的，除了没有检查在回合中早些时候发生过 St。

![image-20211126173501004](C:\Users\longyuan\AppData\Roaming\Typora\typora-user-images\image-20211126173501004.png)

蒙特卡洛控制

   我们考虑经典的策略迭代的蒙特卡洛（MC）版本。这里，我们交替执行策略迭代和策略提升的完整步骤。 从一个随机的策略 π0π0 开始，以最优策略和最优的动作-价值函数结束：

![image-20211126182840193](C:\Users\longyuan\AppData\Roaming\Typora\typora-user-images\image-20211126182840193.png)

其中，→E 表示一个完整的策略评估，→I 表示一个完整的策略提升。

根据广义的策略迭代算法，得到值函数以后，下一步就是进行提升，进而得到最优值函数和最优策略。

为了使过程收敛，我们是建立在两个假设上面的：

1 策略估计过程需要无数个episode才会收敛

2 Exploring starts，即能够保证状态集合S中的所有状态都是有可能被选中为每个episode的初始状态

打破假设1我们有两种办法

![image-20211126183715172](C:\Users\longyuan\AppData\Roaming\Typora\typora-user-images\image-20211126183715172.png)

![image-20211126183732720](C:\Users\longyuan\AppData\Roaming\Typora\typora-user-images\image-20211126183732720.png)

对于蒙特卡洛策略评估而言，以回合制的方式交替使用策略评估和策略提升是很自然的。 每一个回合结束后，观察到的回报用来做策略评估，然后对每个经历的状态做策略提升。 完整的简化算法在下面，我们称作 *探索开端的蒙特卡洛算法* （Monte Carlo ES，即 Monte Carlo with Exploring Starts）。

![image-20211126184426707](C:\Users\longyuan\AppData\Roaming\Typora\typora-user-images\image-20211126184426707.png)

非探索开端的蒙特卡洛控制

