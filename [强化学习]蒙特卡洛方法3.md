# **[强化学习]蒙特卡洛方法**

#### 作者：莫比

### 引言

蒙特卡洛方法并非是一个特定的算法，而是一类随机算法的统称，其基于思想是：用事件发生的“频率”来替代事件发生的“概率”。在机器学习中，这种方法可以用于模型是未知的情况中，它只需要从经验中去学习，这个经验包括样本序列的状态、动作和奖励。得到若干经验后，通过平均所有样本的回报来解决强化学习的任务。

### 1.蒙特卡洛方法的预测问题——策略评估

值函数的求解公式，即回报的期望：
$$
v_\pi(s)=E_\pi[G_t|S_t=s]
$$
但蒙特卡洛方法在策略评估中不是求的回报的期望，而是使用经验平均回报。随着我们的样本越来越多，这个平均值是会收敛于期望的。

也就是说，我们想要估计 $\v\pi(s)$的值，即遵循策略 $\pi$的情况下，状态 $s$ 的价值，我们可以计算所有回合中首次访问状态 $s$ 的平均回报， 以此作为 $\v\pi(s)$ 的估计值，这种方法就是首次访问MC方法； 与之对应的，另一种方法计算所有回合中每次访问状态 $s$ 的平均回报，就是每次访问MC方法。

这两种评估方法去估计$\q_\pi(s,a)$就是求所有episode访问$\(s,a)$所得到回报的均值。

算法流程：

![](https://static.xingzheai.cn/824a322ce0784c0fb1441e337c2b872c.png)

图1.首次访问MC算法

### 2.蒙特卡罗方法的控制问题——策略提升

#### 2.1问题探讨

在我们得到值函数之后，下一步就是进行提升，去获得最优值函数和最优策略。

![](https://static.xingzheai.cn/f650deb37bad40409b18cbac87c7f4b9.jpg)

图2.策略提升的过程

策略提升的方法是对于当前的价值函数，使策略贪婪。我们只需要对每个$s\in\S$选择使动作价值函数最大的那个动作：
$$
\pi(s)=argmax q(s,a)
$$
为了使这个过程收敛，我们必须要建立在下面两个假设上

（a）策略估计过程需要无限个episode才会收敛到回报的期望；

（b）回合都是探索开端的方式，即保证状态集合S中的所有状态都是有可能被选中为每个episode的初始状态。

#### 2.2解决第一个假设

方法一：让每次策略评估都无限接近$\q_{\pi k}$。使用一些方法和一些假设，并且经过足够多的步骤后， 就可以保证一定程度的收敛。

![](https://static.xingzheai.cn/a2a57d5a060e4c4a9142196d9e333eb9.jpg)

图3.方法一的示意图

方法二：在跳转到策略提升前，放弃尝试完成策略评估。评估的每一步，我们将价值函数向$\q_{\pi k}$移动。一个极端的例子是价值迭代，就是每执行一步策略提升就要执行一步迭代策略评估。

![](https://static.xingzheai.cn/03d7823bc1304c09a3a43143da542d12.jpg)

图4.方法二的示意图

#### 2.3解决第二个假设

保证无限次后所有的动作都能被选到的惟一的通用办法是让个体能够持续地选择它们。具体来讲有两种方法，我们称之为在策略(on-policy)方法和离策略（off-policy）。on-policy方法尝试去估计和提升我们用作决策的那个策略；而off-policy估计和提升的策略与用来生成数据的策略不同。

#### 2.3.1 on-policy策略

具体我们使用的是$\epsilon-greedy$策略，这种算法其实就是权衡开发与探索。即在大多数时间选择有最大估计动作价值的动作，但仍有$\epsilon$的概率选择随机的动作。

算法流程：

![](https://static.xingzheai.cn/bb54c34c0d0e4ff1a65d3e806b8c0e2f.jpg)

图5. on-policy首次访问MC控制算法

#### 2.3.2 off-policy策略

off-policy在策略估计和策略提升的时候使用两种策略，一个具有探索性的策略专门用于产生episode积累经验，称为行为策略$\mu$ ，另一个则是用来学习成为最优策略的目标策略 $\pi$  。为了利用从规则 $\mu$  产生的 episodes 来评估 $\pi$  的value，则我们需要规则$\pi$  下的所有行为在规则 $\mu$  下被执行过，也就是要求对所有满足 $\pi(s,a)>0$ 的 $(s,q)$ 均有 $\mu(s,a)>0$ 这个假设可以称为是“覆盖”（coverage)。

#### 2.3.2.1 off-policy策略中的重要性采样

重要性采样就是去估计随机变量在一个分布上的期望值，但是采样的样本来自另一个分布。 离策略上应用重要性采样的方法是，根据目标和行为策略下得到发生的事件轨迹的概率，将得到的回报加权。 两个概率的比值称为重要性采样率。给定初始状态$\S_t$，那么在策略$\pi$下， 接下来的状态动作轨迹 $\A_t,S_{t+1},A_{t+1},\cdots,S_t$ 发生的概率是
$$
\prod_{k=t}^{T-1}{\pi(A_k|S_k)p(S_{k+1}|S_k,A_k)}
$$
其中$p$代表状态转移概率函数。因此，在目标策略和行为策略下的重要性采样率为：
$$
\rho_{t:T-1}=\frac{\prod_{k=t}^{T-1}{\pi(A_k|S_k)p(S_{k+1}|S_k,A_k)}}{\prod_{k=t}^{T-1}{\mu(A_k|S_k)p(S_{k+1}|S_k,A_k)}}=\prod_{k=t}^{T-1}\frac{\pi(A_k|S_k)}{\mu(A_k|S_k)}
$$
从上式可以看出重要性采样率最终仅仅依赖于两个策略和序列，而与MDP无关。接下来就是off-policy评估策略的公式：

（1）原始重要性采样
$$
V(s)=\frac{\sum_{t\in\tau(s)}^{}\rho_{t:T(t)-1}G_t}{|\tau(s)|}
$$
（2）加权重要性采样
$$
V(s)=\frac{\sum_{t\in\tau(s)}^{}\rho_{t:T(t)-1}G_t}{\sum_{t\in\tau(s)}^{}\rho_{t:T(t)-1}}
$$
其中$\tau(s)$:代表所有状态 s 在某个 episode 中第一次被 visit 的时刻的集合

$T(t)$:从时刻t到$T(t)$的回报

$\{G_t\}_{t\in\tau(s)}$：属于状态s的回报

$\{\rho_{t:T-1}\}_{t\in\tau(s)}$：代表相应的重要性采样率

#### 2.3.2.2增量式求均值

假设我们得到了一系列回报$G_1,G_2\cdots,G_t-1$,对于off-policy来说，因为我们利用了重要性采样，所以多了一个权重的因素，设每个回报的权重为 $W_k$
$$
V_n=\frac{\sum_{k=1}^{n-1}W_kG_k}{\sum_{k=1}^{n-1}W_k}
$$
于是有
$$
V_{n+1}=V_n+\frac{W_n}{C_n}(G_n-V_n)
$$

$$
C_{n+1}=C_n+W_{n+1}
$$

#### 2.3.2.3 off-policy策略算法

算法流程：

![](https://static.xingzheai.cn/9129a413473944bb80cc9fe73afe230b.png)

图6. off-policy首次访问MC控制算法

### 3.实例 21点

#### 3.1游戏规则

21点的游戏规则是这样的：游戏里有一个玩家（player）和一个庄家（dealer），每个回合的结果可能是玩家获胜、庄家获胜或打成平手。回合开始时，玩家和庄家各有两张牌，玩家可以看到玩家的两张牌和庄家的其中一张牌。接着，玩家可以选择是不是要更多的牌。如果选择要更多的牌（称为“hit”），玩家可以再得到一张牌，并统计玩家手上所有牌的点数之和。其中牌面A代表1点或11点。如果点数和大于21，则称玩家输掉这一回合，庄家获胜；如果点数和小等于21，那么玩家可以再次决定是否要更多的牌，直到玩家不再要更多的牌。如果玩家在总点数小等于21的情况下不要更多的牌，那么这时候玩家手上的总点数就是最终玩家的点数。接下来，庄家展示其没有显示的那张牌，并且在其点数小于17的情况下抽取更多的牌。如果庄家在抽取的过程中总点数超过21，则庄家输掉这一回合，玩家获胜；如果最终庄家的总点数小于等于21，则比较玩家的总点数和庄家的总点数。如果玩家的总点数大于庄家的总点数，则玩家获胜；如果玩家和庄家的总点数相同，则为平局；如果玩家的总点数小于庄家的总点数，则庄家获胜。

#### 3.2代码实现

这里我使用的是off—policy策略，其实对于蒙特卡罗方法，最主要的就是解决策略评估和策略控制。下面我将给出实验代码：

```python
#策略评估
def evaluate(env, target_policy, behavior_policy, episode_num=500000):
    q = np.zeros_like(target_policy)
    c = np.zeros_like(target_policy)
    for _ in range(episode_num):
        # 用行为策略玩一回合
        state_actions = []
        observation = env.reset()
        while True:
            state = obs2state(observation)
            action = np.random.choice(env. action_space.n, p=behavior_policy[state])
            state_actions.append((state, action))
            obs, reward, done, _ = env.step(action)
            if done:
                break  # 玩好了
        g = reward  # 回报
        rho = 1.  # 重要性采样比率
        for state, action in reversed(state_actions):
            c[state][action] += rho
            q[state][action] += (rho / c[state][action]*(g - q[state][action]))
            rho *= (target_policy[state][action]/ behavior_policy[state][action])
            if rho == 0:
                break  # 提前终止
    return q

#策略控制
def off_policy(env,target_policy,espisode_num=500):
    #柔性策略重要性采样最优策略求解
    q=np.zeros_like(target_policy)
    c=np.zeros_like(target_policy)
    behavior_policy = np.ones_like(target_policy) * 0.5 # soft_policy
    for i in range(espisode_num):
        #用行为策略玩一回合
        state_action=[]
        obs=env.reset()
        while True:
            #使用行为策略生成一回合
            state = obs2state(obs)
            action = np.random.choice(env.action_space.n,p=behavior_policy[state])
            state_action.append(state,action)
            observation, reward, done, _ = env.step(action)
            if done:
                break
            #完成了一个episode
        g=reward
        rho=1#重要性采样比率
        for state,action in reversed(state_action):
            c[state][action]+=rho#C(St,At)←C(St,At)+W
            q[state][action]+=(rho / c[state][action]*(g - q[state][action]))#Q(St,At)←Q(St,At)+W/C(St,At)[G−Q(St,At)]
            #策略提升 π(St)←argmaxaQ(St,a) 
            a =q[state].argmax()
            target_policy[state]=0
            target_policy[state][a]=1
            if a!=action:
                break
        rho /= behavior_policy[state][action]
    return target_policy,q
```

#### 3.3实验结果

![](https://static.xingzheai.cn/4e94908055f746e89ef6a35b5739c704.png)

图7.一个episode

其中一个episode如图，最开始玩家获得[1,4]的牌，庄家显示了5，策略决定要牌（动作为1），这时候玩家的牌就为[1,4,9]，奖励为0，策略继续决定要牌，结果下一回合的观测得到玩家的牌总和为23点，超过21点，所以游戏结束，奖励-1，庄家获胜。

最优策略图和最优价值图如下：

![](https://static.xingzheai.cn/57fc16ffab5e4c749a8fca27bc29aafc.png)

图8.最优策略图

![](https://static.xingzheai.cn/14dd7142d3d944b4be871f12e5bf8f60.png)

图9.最优价值图

with ace：使用了ace，即a当1；without ace：没使用ace，即a当11，紫色到黄色就是从0-1的一个过程。

### 4总结

（a）蒙特卡洛方法是一个用于估计价值函数和发现最优策略的学习方法。与DP不同的是，我们不需要对环境的完全了解。蒙特卡洛方法只需要状态、动作和与环境实际或模拟交互的奖励的经验样本序列。

（b）蒙特卡洛方法是基于平均样本收益来解决强化学习问题的方法。

（c）蒙特卡洛方法对每个状态 - 动作对的回报进行采样和平均。

- [![dOkr60.md.png](https://s1.ax1x.com/2020/08/31/dOkr60.md.png)](https://imgchr.com/i/dOkr60) 

>[行者AI（成都潜在人工智能科技有限公司，xingzhe.ai）](https://xingzhe.ai)致力于使用人工智能和机器学习技术提高游戏和文娱行业的生产力，并持续改善行业的用户体验。我们有内容安全团队、游戏机器人团队、数据平台团队、智能音乐团队和自动化测试团队。 > >如果您对世界拥有强烈的好奇心，不畏惧挑战性问题；能够容忍摸索过程中的各种不确定性、并且坚持下去；能够寻找创新的方式来应对挑战，并同时拥有事无巨细的责任心以确保解决方案的有效执行。那么请将您的个人简历、相关的工作成果及您具体感兴趣的职位提交给我们。
>
>我们欢迎拥抱挑战、并具有创新思维的人才加入我们的团队。请联系：*hr@xingzhe.ai* 
>
>如果您有任何关于内容安全、游戏机器人、数据平台、智能音乐和自动化测试方面的需求，我们也非常荣幸能为您服务。可以联系：*contact@xingzhe.ai*

